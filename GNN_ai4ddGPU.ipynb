{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks for SMall Molecule Regression\n",
    "_by David Holmberg (August 2023)_\n",
    "#### Dataset\n",
    "For this exercise we will use the same dataset of aqueous solubility of 1142 diverse chemical compounds.\n",
    "\n",
    "#### Modelling comparisons\n",
    "1. Compare the results of linear regression to those of Graph Neural Networks\n",
    "\n",
    "#### Aims\n",
    "* Introduce the concept of Graph Neural Networks\n",
    "* Introduce PyTorch code for GNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Pytorch and Pytorch Geometric\n",
    "import torch as tch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch_geometric.nn import GCNConv, GATConv, summary as gsummary, global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import Data#, DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.utils import dropout_adj\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import Lipinski\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "from torchsummary import summary as asummary\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "device = tch.device(\"cuda\" if tch.cuda.is_available() else \"cpu\")\n",
    "# device = tch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions\n",
    "Run these cells to have access to the necessary functions for the lab. Highly encouraged that you read through it and understand, though not necessary for the aims of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_history(train_losses, val_losses, model_name):\n",
    "    fig = plt.figure(figsize=(15, 5), facecolor='w')\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(train_losses)\n",
    "    ax.plot(val_losses)\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(np.log(train_losses))\n",
    "    ax.plot(np.log(val_losses))\n",
    "    ax.set(title=model_name + ': Log model loss', ylabel='Log loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "#Set morgan to 3 or 4 and nBits to 1024\n",
    "def smiles_to_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
    "    return list(fp.ToBitString())\n",
    "\n",
    "def smiles_to_mol(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    # if mol is not None:\n",
    "    #     mol = Chem.AddHs(mol)\n",
    "    return mol\n",
    "\n",
    "def read_smiles_data(path_data):\n",
    "    df = pd.read_csv(path_data, sep=',')\n",
    "    df['fingerprint'] = df['SMILES'].apply(smiles_to_fingerprint)\n",
    "    df['fingerprint'] = df['fingerprint'].apply(lambda x: [int(bit) for bit in x])\n",
    "    df['fingerprint'] = df['fingerprint'].apply(lambda x: np.array(x))\n",
    "    df['mol'] = df['SMILES'].apply(smiles_to_mol)\n",
    "    return df\n",
    "\n",
    "def is_hydrogen_donor(atomic_num, hybridization):\n",
    "    return int((atomic_num == 8 or atomic_num == 7) and (hybridization == 3 or hybridization == 2))\n",
    "\n",
    "def is_polar_bond(atom1_num, atom2_num, electronegativity):\n",
    "    en1 = electronegativity.get(atom1_num, None)\n",
    "    en2 = electronegativity.get(atom2_num, None)\n",
    "    if en1 is None or en2 is None:\n",
    "        return 0  # Unknown electronegativity, consider as non-polar\n",
    "    return int(abs(en1 - en2) > 0.4)\n",
    "\n",
    "def electroneg():\n",
    "    return {\n",
    "    1: 2.20,  # H\n",
    "    3: 0.98,  # Li\n",
    "    4: 1.57,  # Be\n",
    "    5: 2.04,  # B\n",
    "    6: 2.55,  # C\n",
    "    7: 3.04,  # N\n",
    "    8: 3.44,  # O\n",
    "    9: 3.98,  # F\n",
    "    11: 0.93, # Na\n",
    "    12: 1.31, # Mg\n",
    "    13: 1.61, # Al\n",
    "    14: 1.90, # Si\n",
    "    15: 2.19, # P\n",
    "    16: 2.58, # S\n",
    "    17: 3.16, # Cl\n",
    "    19: 0.82, # K\n",
    "    20: 1.00, # Ca\n",
    "    22: 1.54, # Ti\n",
    "    24: 1.66, # Cr\n",
    "    25: 1.55, # Mn\n",
    "    26: 1.83, # Fe\n",
    "    27: 1.88, # Co\n",
    "    28: 1.91, # Ni\n",
    "    29: 1.90, # Cu\n",
    "    30: 1.65, # Zn\n",
    "    35: 2.96, # Br\n",
    "    53: 2.66, # I\n",
    "}\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, (GCNConv, GATConv)):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "def make_pyg(row):\n",
    "    # Create node features\n",
    "    mol = row['mol']\n",
    "    # pauling = electroneg()\n",
    "    atom_num = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
    "    atom_hyb = [int(atom.GetHybridization()) for atom in mol.GetAtoms()]\n",
    "    atom_deg = [atom.GetDegree() for atom in mol.GetAtoms()]\n",
    "    atom_arom = [int(atom.GetIsAromatic()) for atom in mol.GetAtoms()]  # Aromaticity\n",
    "    atom_hydrogens = [atom.GetTotalNumHs() for atom in mol.GetAtoms()]  # Number of hydrogens\n",
    "    # atom_h_donor = [is_hydrogen_donor(num, hyb) for num, hyb in zip(atom_num, atom_hyb)]\n",
    "    atom_charge = [atom.GetFormalCharge() for atom in mol.GetAtoms()]  # Formal charge\n",
    "    atom_chiral_tag = [int(atom.GetChiralTag()) for atom in mol.GetAtoms()]  # Chirality\n",
    "    atom_val = [atom.GetExplicitValence() for atom in mol.GetAtoms()]\n",
    "    #atom_mass = [atom.GetMass() for atom in mol.GetAtoms()]\n",
    "    #atom_pauling = [pauling.get(num, 0) for num in atom_num]\n",
    "    \n",
    "    x1 = tch.tensor(atom_num, dtype=tch.float).view(-1, 1)\n",
    "    x2 = tch.tensor(atom_hyb, dtype=tch.float).view(-1, 1)\n",
    "    x3 = tch.tensor(atom_deg, dtype=tch.float).view(-1, 1)\n",
    "    x4 = tch.tensor(atom_arom, dtype=tch.float).view(-1, 1)\n",
    "    x5 = tch.tensor(atom_hydrogens, dtype=tch.float).view(-1, 1)\n",
    "    x6 = tch.tensor(atom_charge, dtype=tch.float).view(-1, 1)\n",
    "    x7 = tch.tensor(atom_chiral_tag, dtype=tch.float).view(-1, 1)\n",
    "    x8 = tch.tensor(atom_val, dtype=tch.float).view(-1, 1)\n",
    "    # x9 = tch.tensor(atom_h_donor, dtype=tch.float).view(-1, 1)\n",
    "    #x10 = tch.tensor(atom_mass, dtype=tch.float).view(-1, 1)\n",
    "    #x11 = tch.tensor(atom_pauling, dtype=tch.float).view(-1, 1)\n",
    "    \n",
    "    y = tch.tensor(row['measured.log.solubility.mol.L.'], dtype=tch.float).view(-1, 1)\n",
    "    x = tch.cat([x1\n",
    "                 , x2\n",
    "                 , x3\n",
    "                 , x4\n",
    "                 , x5\n",
    "                 , x6\n",
    "                 , x7\n",
    "                 , x8\n",
    "                # , x9\n",
    "                # , x10\n",
    "                 #, x11\n",
    "                 ], dim=1)\n",
    "    \n",
    "    # Create edge features (connectivity)\n",
    "    edge_indices = []\n",
    "    edge_features = []\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_indices.append((i, j))\n",
    "        bond_type = bond.GetBondTypeAsDouble()\n",
    "        is_conjugated = int(bond.GetIsConjugated())  # Conjugation\n",
    "        is_in_ring = int(bond.IsInRing())  # Ring membership\n",
    "        bond_stereo = int(bond.GetStereo())  # Stereo configuration\n",
    "        #bond_polarity = is_polar_bond(atom_num[i], atom_num[j], pauling)\n",
    "\n",
    "        edge_features.append([bond_type\n",
    "                              , is_conjugated\n",
    "                              , is_in_ring\n",
    "                              , bond_stereo\n",
    "                              #, bond_polarity\n",
    "                              ])\n",
    "    \n",
    "    edge_index = tch.tensor(edge_indices, dtype=tch.long).t().contiguous()\n",
    "    edge_attr = tch.tensor(edge_features, dtype=tch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "\n",
    "    data.smiles = row['SMILES']\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions for Neural Networks\n",
    "This cell defines the training methods for the neural networks you will use later in this notebook. WHile they are ready to use, you should read them carefully to understand what parameters you will need to use, and how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitGNN(gnn1_model, t_loader, v_loader, num_epochs, optimizer, criterion, scheduler):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    pbar = tqdm(range(num_epochs), desc=\"Epochs\")\n",
    "    pbar.reset()\n",
    "    pbar_t = tqdm(total=len(t_loader), desc=\"Training Batch:\", leave=False)\n",
    "    pbar_v = tqdm(total=len(v_loader), desc=\"validation Batch:\", leave=False)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        gnn1_model.train()\n",
    "        train_loss_items = []\n",
    "        pbar_t.reset()\n",
    "        pbar_v.reset()\n",
    "        for batch in t_loader:\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Use Batch Data object in forward pass\n",
    "            outputs = gnn1_model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "            loss = criterion(outputs, batch.y)\n",
    "\n",
    "            l1_lambda = 0.0001\n",
    "            l1_norm = sum(p.abs().sum() for p in gnn1_model.parameters())\n",
    "            loss += l1_lambda * l1_norm\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_items.append(loss.item())\n",
    "            pbar_t.update()\n",
    "        avg_train_loss = sum(train_loss_items) / len(train_loss_items)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        # Validation Phase (assuming you have a separate validation loader)\n",
    "        gnn1_model.eval()\n",
    "        val_loss_items = []\n",
    "        with tch.no_grad():\n",
    "            for val_batch in v_loader:\n",
    "                val_batch.to(device)\n",
    "                val_outputs = gnn1_model(val_batch.x.float(), val_batch.edge_index, val_batch.batch)\n",
    "                val_loss = criterion(val_outputs, val_batch.y)\n",
    "                val_loss_items.append(val_loss.item())\n",
    "                pbar_v.update()\n",
    "\n",
    "        avg_val_loss = sum(val_loss_items) / len(val_loss_items)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_losses[-1]:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\"Training Loss\": avg_train_loss, \"Validation Loss\": avg_val_loss})\n",
    "        scheduler.step()\n",
    "    return gnn1_model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_smiles_data('data/solubility.csv')\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['fingerprint'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# df['measured.log.solubility.mol.L.'] = scaler.fit_transform(df[['measured.log.solubility.mol.L.']])\n",
    "data_pyg = df.apply(make_pyg, axis=1)\n",
    "data_pyg = data_pyg[data_pyg.apply(lambda x: len(x.edge_index.shape) != 1)]\n",
    "data_pyg.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_df = int(len(df) * 0.7)\n",
    "df_train = df.iloc[:n_df]\n",
    "# df_train = df.sample(frac=0.8)\n",
    "df_test = df.drop(df_train.index)\n",
    "\n",
    "df_train.reset_index(drop=True)\n",
    "df_test.reset_index(drop=True)\n",
    "X_train, y_train = df_train['fingerprint'].tolist(), df_train['measured.log.solubility.mol.L.'].tolist()\n",
    "X_test, y_test = df_test['fingerprint'].tolist(), df_test['measured.log.solubility.mol.L.'].tolist()\n",
    "\n",
    "n_train = int(len(data_pyg) * 0.7) # 70% of data for training and 30% for testing\n",
    "indices = np.arange(n_train)\n",
    "data_train = data_pyg[indices[:n_train]]\n",
    "data_train.reset_index(drop=True, inplace=True)\n",
    "data_test = data_pyg[~data_pyg.isin(data_train)]\n",
    "data_test.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RandomForestRegressor()\n",
    "RF_model.fit(X_train, y_train)\n",
    "RF_pred = RF_model.predict(X_test)\n",
    "RF_mse = mean_squared_error(y_test, RF_pred)\n",
    "print('Random Forest Regressor: MSE = ' + str(np.round(RF_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SV_model = SVR()\n",
    "SV_model.fit(X_train, y_train)\n",
    "SV_pred = SV_model.predict(X_test)\n",
    "SV_mse = mean_squared_error(y_test, SV_pred)\n",
    "print('Support Vector Regressor: MSE = ' + str(np.round(SV_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data_train.iloc[5].x.size(1)\n",
    "print('Input Dimensions: ', input_dim)\n",
    "#Loss, Epochs, Batch-size\n",
    "num_epochs = 10000\n",
    "batch_size = 64\n",
    "sched_size = int(num_epochs//5)\n",
    "weight_decay = 1e-4\n",
    "gamma = 0.81\n",
    "criterion = nn.MSELoss()\n",
    "#Data Loaders to handle the graphs we made earlier\n",
    "t_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "v_loader = DataLoader(data_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN1(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GNN1, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, 32)\n",
    "        self.conv2 = GCNConv(32, 64)\n",
    "        self.conv3 = GCNConv(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)  # Output layer with 1 node\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv3(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn1_model = GNN1(input_dim)\n",
    "gnn1_model = gnn1_model.to(device)\n",
    "# Adam adjusts learning rate as needed\n",
    "optimizer = optim.AdamW(gnn1_model.parameters(), lr=0.0001,weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=sched_size, gamma=gamma)\n",
    "#Traing the Model\n",
    "gnn1_model, train_losses, val_losses = fitGNN(gnn1_model, t_loader, v_loader, num_epochs, optimizer, criterion, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn1_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with tch.no_grad():\n",
    "    for v_batch in v_loader:\n",
    "        v_batch.to(device)\n",
    "        test_outputs = gnn1_model(v_batch.x.float(), v_batch.edge_index, v_batch.batch)\n",
    "        all_preds.extend(test_outputs.tolist())\n",
    "        all_labels.extend(v_batch.y.tolist())\n",
    "all_preds_tensor = tch.tensor(all_preds)\n",
    "all_labels_tensor = tch.tensor(all_labels)\n",
    "gnn1_mse = mean_squared_error(all_labels_tensor, all_preds_tensor)\n",
    "\n",
    "print(f'GNN1 Regression: MSE = {gnn1_mse:.3f}')\n",
    "plot_history(train_losses, val_losses, 'GNN1 Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GNN2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GNN2, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, 32, heads=1, concat=True)  # Two attention heads\n",
    "        self.conv2 = GATConv(32, 32, heads=2, concat=True)  # Two attention heads\n",
    "        self.conv3 = GATConv(64, 8, heads=2, concat=True)  # Two attention heads\n",
    "        self.fc3 = nn.Linear(16, 1)  # Output layer with 1 node\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv3(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gnn2_model = GNN2(input_dim).to(device)\n",
    "optimizer = optim.AdamW(gnn2_model.parameters()\n",
    "                       , lr=0.001\n",
    "                       #, weight_decay= weight_decay\n",
    "                       )  # Adjust learning rate as needed\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=sched_size, gamma=gamma)\n",
    "gnn2_model, train_losses, val_losses = fitGNN(gnn2_model, t_loader, v_loader, num_epochs, optimizer, criterion, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn2_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with tch.no_grad():\n",
    "    for v_batch in v_loader:\n",
    "        v_batch.to(device)\n",
    "        test_outputs = gnn2_model(v_batch.x, v_batch.edge_index, v_batch.batch)\n",
    "        all_preds.extend(test_outputs.tolist())\n",
    "        all_labels.extend(v_batch.y.tolist())\n",
    "all_preds_tensor = tch.tensor(all_preds)\n",
    "all_labels_tensor = tch.tensor(all_labels)\n",
    "gnn2_mse = mean_squared_error(all_labels_tensor, all_preds_tensor)\n",
    "\n",
    "print(f'GNN2 Regression: MSE = {gnn2_mse:.3f}')\n",
    "plot_history(train_losses, val_losses, 'GNN2 Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN3(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GNN3, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, 32, heads=1, concat=True)  # Single attention head\n",
    "        self.conv2 = GATConv(32, 32, heads=2, concat=True)  # Single attention head\n",
    "        self.conv3 = GATConv(64, 64, heads=2, concat=True)\n",
    "        self.conv4 = GATConv(128, 32, heads=2, concat=True)  # Single attention head\n",
    "        self.conv5 = GATConv(64, 8, heads=2, concat=True)  # Single attention head  # Single attention head\n",
    "        self.fc3 = nn.Linear(16, 1)  # Output layer with 1 node\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.35)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv3(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv4(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv5(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GNN1 model\n",
    "gnn3_model = GNN3(input_dim).to(device)\n",
    "optimizer = optim.AdamW(gnn3_model.parameters()\n",
    "                       , lr=0.001\n",
    "                       , weight_decay= weight_decay*15\n",
    "                       )  # Adjust learning rate as needed\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=sched_size, gamma=gamma)\n",
    "gnn3_model, train_losses, val_losses = fitGNN(gnn3_model, t_loader, v_loader, num_epochs, optimizer, criterion, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn3_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with tch.no_grad():\n",
    "    for v_batch in v_loader:\n",
    "        v_batch.to(device)\n",
    "        test_outputs = gnn3_model(v_batch.x, v_batch.edge_index, v_batch.batch)\n",
    "        all_preds.extend(test_outputs.tolist())\n",
    "        all_labels.extend(v_batch.y.tolist())\n",
    "all_preds_tensor = tch.tensor(all_preds)\n",
    "all_labels_tensor = tch.tensor(all_labels)\n",
    "gnn3_mse = mean_squared_error(all_labels_tensor, all_preds_tensor)\n",
    "\n",
    "print(f'GNN3 Regression: MSE = {gnn3_mse:.3f}')\n",
    "plot_history(train_losses, val_losses, 'GNN3 Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([\"SVM\", \"Random Forest\", \"GNN1\", \"GNN2\", \"GNN3\"], [SV_mse, RF_mse, gnn1_mse, gnn2_mse, gnn3_mse])\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
