{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks for SMall Molecule Regression\n",
    "_by David Holmberg (August 2023)_\n",
    "#### Dataset\n",
    "For this exercise we will use the same dataset of aqueous solubility of 1142 diverse chemical compounds.\n",
    "\n",
    "#### Modelling comparisons\n",
    "1. Compare the results of linear regression to those of Graph Neural Networks\n",
    "\n",
    "#### Aims\n",
    "* Introduce the concept of Graph Neural Networks\n",
    "* Introduce PyTorch code for GNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Pytorch and Pytorch Geometric\n",
    "import torch as tch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.nn import GCNConv, GATConv, summary as gsummary, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Helper libraries\n",
    "from torchsummary import summary as asummary\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# device = tch.device(\"cuda\" if tch.cuda.is_available() else \"cpu\")\n",
    "tch.backends.cudnn.enabled = False\n",
    "tch.cuda.is_available = lambda : False\n",
    "device = tch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions\n",
    "Run these cells to have access to the necessary functions for the lab. Highly encouraged that you read through it and understand, though not necessary for the aims of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_history(train_losses, val_losses, model_name):\n",
    "    fig = plt.figure(figsize=(15, 5), facecolor='w')\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(train_losses)\n",
    "    ax.plot(val_losses)\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(np.log(train_losses))\n",
    "    ax.plot(np.log(val_losses))\n",
    "    ax.set(title=model_name + ': Log model loss', ylabel='Log loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "#Set morgan to 3 or 4 and nBits to 1024\n",
    "def smiles_to_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
    "    return list(fp.ToBitString())\n",
    "\n",
    "def smiles_to_mol(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return mol\n",
    "\n",
    "def read_smiles_data(path_data):\n",
    "    df = pd.read_csv(path_data, sep=',')\n",
    "    df['fingerprint'] = df['SMILES'].apply(smiles_to_fingerprint)\n",
    "    df['fingerprint'] = df['fingerprint'].apply(lambda x: [int(bit) for bit in x])\n",
    "    df['fingerprint'] = df['fingerprint'].apply(lambda x: np.array(x))\n",
    "    df['mol'] = df['SMILES'].apply(smiles_to_mol)\n",
    "    return df\n",
    "\n",
    "def make_pyg(row):\n",
    "    # Create node features\n",
    "    mol = row['mol']\n",
    "    atom_num = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
    "    atom_hyb = [int(atom.GetHybridization()) for atom in mol.GetAtoms()]\n",
    "    atom_deg = [atom.GetDegree() for atom in mol.GetAtoms()]\n",
    "    atom_arom = [int(atom.GetIsAromatic()) for atom in mol.GetAtoms()]  # Aromaticity\n",
    "    atom_hydrogens = [atom.GetTotalNumHs() for atom in mol.GetAtoms()]  # Number of hydrogens\n",
    "    atom_charge = [atom.GetFormalCharge() for atom in mol.GetAtoms()]  # Formal charge\n",
    "    atom_chiral_tag = [int(atom.GetChiralTag()) for atom in mol.GetAtoms()]  # Chirality\n",
    "    atom_val = [atom.GetExplicitValence() for atom in mol.GetAtoms()]\n",
    "    \n",
    "    x1 = tch.tensor(atom_num, dtype=tch.float).view(-1, 1)\n",
    "    x2 = tch.tensor(atom_hyb, dtype=tch.float).view(-1, 1)\n",
    "    x3 = tch.tensor(atom_deg, dtype=tch.float).view(-1, 1)\n",
    "    x4 = tch.tensor(atom_arom, dtype=tch.float).view(-1, 1)\n",
    "    x5 = tch.tensor(atom_hydrogens, dtype=tch.float).view(-1, 1)\n",
    "    x6 = tch.tensor(atom_charge, dtype=tch.float).view(-1, 1)\n",
    "    x7 = tch.tensor(atom_chiral_tag, dtype=tch.float).view(-1, 1)\n",
    "    x8 = tch.tensor(atom_val, dtype=tch.float).view(-1, 1)\n",
    "    \n",
    "    y = tch.tensor(row['measured.log.solubility.mol.L.'], dtype=tch.float).view(-1, 1)\n",
    "    x = tch.cat([x1, x2, x3, x4, x5, x6, x7, x8], dim=1)\n",
    "    \n",
    "    # Create edge features (connectivity)\n",
    "    edge_indices = []\n",
    "    edge_features = []\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_indices.append((i, j))\n",
    "        bond_type = bond.GetBondTypeAsDouble()\n",
    "        is_conjugated = int(bond.GetIsConjugated())  # Conjugation\n",
    "        is_in_ring = int(bond.IsInRing())  # Ring membership\n",
    "        bond_stereo = int(bond.GetStereo())  # Stereo configuration\n",
    "        \n",
    "        edge_features.append([bond_type, is_conjugated, is_in_ring, bond_stereo])\n",
    "    \n",
    "    edge_index = tch.tensor(edge_indices, dtype=tch.long).t().contiguous()\n",
    "    edge_attr = tch.tensor(edge_features, dtype=tch.float)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions for Neural Networks\n",
    "This cell defines the training methods for the neural networks you will use later in this notebook. WHile they are ready to use, you should read them carefully to understand what parameters you will need to use, and how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitGNN(gnn1_model, t_loader, v_loader, num_epochs, batch_size, optimizer, criterion):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    pbar = tqdm(range(num_epochs), desc=\"Epochs\")\n",
    "    pbar.reset()\n",
    "    pbar_t = tqdm(total=len(t_loader), desc=\"Training Batch:\", leave=False)\n",
    "    pbar_v = tqdm(total=len(v_loader), desc=\"validation Batch:\", leave=False)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        gnn1_model.train()\n",
    "        train_loss_items = []\n",
    "        pbar_t.reset()\n",
    "        pbar_v.reset()\n",
    "        for batch in t_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # Use Batch Data object in forward pass\n",
    "            outputs = gnn1_model(batch.x, batch.edge_index, batch.batch)\n",
    "            loss = criterion(outputs, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_items.append(loss.item())\n",
    "            pbar_t.update()\n",
    "        avg_train_loss = sum(train_loss_items) / len(train_loss_items)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        # Validation Phase (assuming you have a separate validation loader)\n",
    "        gnn1_model.eval()\n",
    "        val_loss_items = []\n",
    "        with tch.no_grad():\n",
    "            for val_batch in v_loader:\n",
    "                val_outputs = gnn1_model(val_batch.x, val_batch.edge_index, val_batch.batch)\n",
    "                val_loss = criterion(val_outputs, val_batch.y)\n",
    "                val_loss_items.append(val_loss.item())\n",
    "                pbar_v.update()\n",
    "\n",
    "        avg_val_loss = sum(val_loss_items) / len(val_loss_items)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_losses[-1]:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\"Training Loss\": avg_train_loss, \"Validation Loss\": avg_val_loss})\n",
    "    return gnn1_model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and check shape of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Compound.ID</th>\n",
       "      <th>measured.log.solubility.mol.L.</th>\n",
       "      <th>ESOL.predicted.log.solubility.mol.L.</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>fingerprint</th>\n",
       "      <th>mol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RTI 19</td>\n",
       "      <td>-4.749</td>\n",
       "      <td>-4.007</td>\n",
       "      <td>CCN2c1ccccc1N(C)C(=O)c3ccccc23</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7fe8700cbe80&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3-Methylpentane</td>\n",
       "      <td>-3.680</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>CCC(C)CC</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7fe8700a45e0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Altretamine</td>\n",
       "      <td>-3.364</td>\n",
       "      <td>-2.492</td>\n",
       "      <td>CN(C)c1nc(nc(n1)N(C)C)N(C)C</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7fe8700bc520&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dioxacarb</td>\n",
       "      <td>-1.570</td>\n",
       "      <td>-1.614</td>\n",
       "      <td>CNC(=O)Oc1ccccc1C2OCCO2</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7fe8700c1dc0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1,1-Dichloroethylene</td>\n",
       "      <td>-1.640</td>\n",
       "      <td>-1.939</td>\n",
       "      <td>ClC(=C)Cl</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7fe8700bb5e0&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Compound.ID  measured.log.solubility.mol.L.   \n",
       "0                RTI 19                          -4.749  \\\n",
       "1       3-Methylpentane                          -3.680   \n",
       "2           Altretamine                          -3.364   \n",
       "3             Dioxacarb                          -1.570   \n",
       "4  1,1-Dichloroethylene                          -1.640   \n",
       "\n",
       "   ESOL.predicted.log.solubility.mol.L.                           SMILES   \n",
       "0                                -4.007  CCN2c1ccccc1N(C)C(=O)c3ccccc23   \\\n",
       "1                                -2.600                         CCC(C)CC   \n",
       "2                                -2.492      CN(C)c1nc(nc(n1)N(C)C)N(C)C   \n",
       "3                                -1.614          CNC(=O)Oc1ccccc1C2OCCO2   \n",
       "4                                -1.939                        ClC(=C)Cl   \n",
       "\n",
       "                                         fingerprint   \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \\\n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                mol  \n",
       "0  <rdkit.Chem.rdchem.Mol object at 0x7fe8700cbe80>  \n",
       "1  <rdkit.Chem.rdchem.Mol object at 0x7fe8700a45e0>  \n",
       "2  <rdkit.Chem.rdchem.Mol object at 0x7fe8700bc520>  \n",
       "3  <rdkit.Chem.rdchem.Mol object at 0x7fe8700c1dc0>  \n",
       "4  <rdkit.Chem.rdchem.Mol object at 0x7fe8700bb5e0>  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_smiles_data('data/solubility.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['fingerprint'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# df['measured.log.solubility.mol.L.'] = scaler.fit_transform(df[['measured.log.solubility.mol.L.']])\n",
    "data_pyg = df.apply(make_pyg, axis=1)\n",
    "data_pyg = data_pyg[data_pyg.apply(lambda x: len(x.edge_index.shape) != 1)]\n",
    "data_pyg.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and test sets and standardize the data\n",
    "Here we will just have a training and test set, so our results will not be quite as rigerous as those you got with cross-validation in the supervised machine learning lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_train = int(len(y) * 0.7) # 70% of data for training and 30% for testingS\n",
    "\n",
    "# random.seed(1234)\n",
    "# indices = np.arange(len(y))\n",
    "# random.shuffle(indices)\n",
    "\n",
    "# # X_train0 is our training data prior to standardization\n",
    "# X_train, X_test = X[indices[:n_train]], X[indices[n_train:]]\n",
    "# y_train, y_test = y[indices[:n_train]], y[indices[n_train:]]\n",
    "\n",
    "df_train = df.sample(frac=0.7)\n",
    "df_test = df.drop(df_train.index)\n",
    "\n",
    "df_train.reset_index(drop=True)\n",
    "df_test.reset_index(drop=True)\n",
    "X_train, y_train = df_train['fingerprint'].tolist(), df_train['measured.log.solubility.mol.L.'].tolist()\n",
    "X_test, y_test = df_test['fingerprint'].tolist(), df_test['measured.log.solubility.mol.L.'].tolist()\n",
    "\n",
    "# Here we split the molecular data into a train and a test set\n",
    "n_train = int(len(data_pyg) * 0.7) # 70% of data for training and 30% for testing\n",
    "# random.seed(1234)\n",
    "indices = np.arange(n_train)\n",
    "# print(indices[-1])\n",
    "data_train = data_pyg[indices[:n_train]]\n",
    "data_train.reset_index(drop=True, inplace=True)\n",
    "data_test = data_pyg[~data_pyg.isin(data_train)]\n",
    "data_test.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "## Random Forest Regressor & Support Vector Regressor\n",
    "For comparative purposes, with the results we will explore later with a more involved neural network architectures than the one above, we will build a Linear Regression, Random Forest and Support Vector model. For these three machine learning algorithms we will just use the default hyper parameter settings, which are often a good place to start. This means that you will just have () after the model definition, as you did for the linear regression with LinearRegression(). To change the hyper parameters from the defaults one needs to specify them within the braces.\n",
    "\n",
    "The code cells for the random forest and support vector regressors have been left blank below. You should fill in these cells. You should define the models, fit them, make predictions from them, compute their MSEs and print out the results.\n",
    "\n",
    "* hint 1: look to the cell where we 'Load packages' to get the right model definition for the two machine learning methods\n",
    "* hint 2: look at the cell with Linear Regression. It should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: MSE = 1.3174048937513943e+19\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression\n",
    "LR_model = LinearRegression()\n",
    "LR_model.fit(X_train, y_train)\n",
    "LR_pred = LR_model.predict(X_test)\n",
    "LR_mse = mean_squared_error(y_test, LR_pred)\n",
    "print('Linear Regression: MSE = ' + str(np.round(LR_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor: MSE = 1.678\n"
     ]
    }
   ],
   "source": [
    "RF_model = RandomForestRegressor()\n",
    "RF_model.fit(X_train, y_train)\n",
    "RF_pred = RF_model.predict(X_test)\n",
    "RF_mse = mean_squared_error(y_test, RF_pred)\n",
    "print('Random Forest Regressor: MSE = ' + str(np.round(RF_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Regressor: MSE = 1.852\n"
     ]
    }
   ],
   "source": [
    "SV_model = SVR()\n",
    "SV_model.fit(X_train, y_train)\n",
    "SV_pred = SV_model.predict(X_test)\n",
    "SV_mse = mean_squared_error(y_test, SV_pred)\n",
    "print('Support Vector Regressor: MSE = ' + str(np.round(SV_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing GNNs\n",
    "So, now you've tested regression on molecular descriptors with Regular ML. ANother option that is gaining traction in the research world is using Graph Neural Networks. You will be using an extension library called Pytorch.Geometric for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN1(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GNN1, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, 16)\n",
    "        self.conv2 = GCNConv(16, 8)\n",
    "        # self.conv3 = GCNConv(16, 8)\n",
    "        # self.conv4 = GCNConv(32, 16)\n",
    "        self.fc3 = nn.Linear(8, 1)  # Output layer with 1 node\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        # x = self.relu(self.conv3(x, edge_index))\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.relu(self.conv4(x, edge_index))\n",
    "        # x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GNN1 model\n",
    "input_dim = data_train.iloc[5].x.size(1)\n",
    "print('Input Dimensions: ', input_dim)\n",
    "gnn1_model = GNN1(input_dim)\n",
    "#Loss and optimizer|\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gnn1_model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
    "t_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "v_loader = DataLoader(data_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn1_model, train_losses, val_losses = fitGNN(gnn1_model, t_loader, v_loader, num_epochs, batch_size, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn1_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with tch.no_grad():\n",
    "    for v_batch in v_loader:\n",
    "        test_outputs = gnn1_model(v_batch.x, v_batch.edge_index, v_batch.batch)\n",
    "        all_preds.extend(test_outputs.tolist())\n",
    "        all_labels.extend(v_batch.y.tolist())\n",
    "all_preds_tensor = tch.tensor(all_preds)\n",
    "all_labels_tensor = tch.tensor(all_labels)\n",
    "gnn1_mse = mean_squared_error(all_labels_tensor, all_preds_tensor)\n",
    "\n",
    "print(f'GNN1 Regression: MSE = {gnn1_mse:.3f}')\n",
    "plot_history(train_losses, val_losses, 'GNN1 Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GNN2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GNN2, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, 16, heads=1, concat=True)  # Single attention head\n",
    "        self.conv2 = GATConv(16, 8, heads=1, concat=True)  # Single attention head\n",
    "        self.fc3 = nn.Linear(8, 1)  # Output layer with 1 node\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GNN1 model\n",
    "input_dim = data_train.iloc[5].x.size(1)\n",
    "print('Input Dimensions: ', input_dim)\n",
    "gnn2_model = GNN2(input_dim)\n",
    "#Loss and optimizer|\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gnn2_model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
    "t_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "v_loader = DataLoader(data_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "gnn2_model, train_losses, val_losses = fitGNN(gnn2_model, t_loader, v_loader, num_epochs, batch_size, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn2_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with tch.no_grad():\n",
    "    for v_batch in v_loader:\n",
    "        test_outputs = gnn2_model(v_batch.x, v_batch.edge_index, v_batch.batch)\n",
    "        all_preds.extend(test_outputs.tolist())\n",
    "        all_labels.extend(v_batch.y.tolist())\n",
    "all_preds_tensor = tch.tensor(all_preds)\n",
    "all_labels_tensor = tch.tensor(all_labels)\n",
    "gnn1_mse = mean_squared_error(all_labels_tensor, all_preds_tensor)\n",
    "\n",
    "print(f'GNN2 Regression: MSE = {gnn1_mse:.3f}')\n",
    "plot_history(train_losses, val_losses, 'GNN2 Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GNN2(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(GNN2, self).__init__()\n",
    "#         self.conv1 = GATConv(input_dim, 8, heads=2, concat=True)  # 2 heads, 8*2=16 output dimensions\n",
    "#         self.conv2 = GATConv(16, 4, heads=2, concat=True)  # 2 heads, 4*2=8 output dimensions\n",
    "#         self.fc3 = nn.Linear(8, 1)  # Output layer with 1 node\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         x = self.relu(self.conv1(x, edge_index))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.relu(self.conv2(x, edge_index))\n",
    "#         x = self.dropout(x)\n",
    "#         x = global_mean_pool(x, batch)\n",
    "#         x = self.fc3(x)\n",
    "#         return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
