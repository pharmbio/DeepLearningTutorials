{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'markdown cell' below  replace the `???` with the names of those in your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neural networks for QSAR\n",
    "_by David Holmberg (August 2023)_\n",
    "#### Dataset\n",
    "For this exercise we will use the same dataset of aqueous solubility of 1142 diverse chemical compounds as you previously explored during the QSAR lab last week. However, here we will only use the PhysChem descriptors.\n",
    "\n",
    "#### Modelling comparisons\n",
    "1. Compare the results of linear regression to those of a simple neural network with no hidden layers and no non-linear activation functions\n",
    "2. Compare the results of a a random forest regressor, a support vector regressor, and a neural network with two hidden layers (with non-linear activations) and dropout.\n",
    "\n",
    "#### Aims\n",
    "* to see the link between neural networks and linear regression\n",
    "* to learn the basics of how to define, compile, fit and evaluate neural networks via TensorFlow.\n",
    "\n",
    "#### Note\n",
    "We will be using the open-source machine learning framework TensorFlow (https://www.tensorflow.org) and Keras (https://keras.io) for our neural networks. TensorFlow was developed by the Google Brain team and is today one of the most widely used machine learning frameworks in research and industry and Keras was/is the most popular higher-level API that runs atop TensorFlow. However, last year TensorFlow 2 was released. In TensorFow 2 (which we will use for all our neural network work) Keras is now fully integrated. This means that we get all the benefits of TensorFlow with a much easier (Keras-type) way to define and train models than was previously possible with TensorFlow 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Pytorch and Pytorch Geometric\n",
    "import torch as tch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.nn import GCNConv, summary as gsummary, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Helper libraries\n",
    "from torchsummary import summary as asummary\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# tch.backends.cudnn.enabled = False\n",
    "# tch.cuda.is_available = lambda : False\n",
    "# device = tch.device('cpu')\n",
    "!nvidia-smi | grep GeForce\n",
    "device = tch.device(\"cuda\" if tch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions\n",
    "Run these cells to have access to the necessary functions for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_history(train_losses, val_losses, model_name):\n",
    "    fig = plt.figure(figsize=(15, 5), facecolor='w')\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(train_losses)\n",
    "    ax.plot(val_losses)\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(np.log(train_losses))\n",
    "    ax.plot(np.log(val_losses))\n",
    "    ax.set(title=model_name + ': Log model loss', ylabel='Log loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def smiles_to_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=128)\n",
    "    return list(fp.ToBitString())\n",
    "\n",
    "def smiles_to_mol(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return mol\n",
    "\n",
    "def read_smiles_data(path_data):\n",
    "    df = pd.read_csv(path_data, sep=',')\n",
    "    df['fingerprint'] = df['SMILES'].apply(smiles_to_fingerprint)\n",
    "    df['fingerprint'] = df['fingerprint'].apply(lambda x: [int(bit) for bit in x])\n",
    "    df['mol'] = df['SMILES'].apply(smiles_to_mol)\n",
    "    return df\n",
    "\n",
    "def make_pyg(row):\n",
    "    # Create node features\n",
    "    mol = row['mol']\n",
    "    atom_features = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
    "    x1 = tch.tensor(atom_features, dtype=tch.float).view(-1, 1)\n",
    "    x2 = tch.tensor(row['fingerprint'], dtype=tch.float)\n",
    "    y = tch.tensor(row['measured.log.solubility.mol.L.'], dtype=tch.float).view(-1, 1)\n",
    "    # Duplicate fingerprint for each atom and concatenate\n",
    "    x2_repeated = x2.repeat(x1.shape[0], 1)\n",
    "    x = tch.cat([x1, x2_repeated], dim=1)\n",
    "    \n",
    "    # Create edge features (connectivity)\n",
    "    edge_indices = []\n",
    "    edge_features = []\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_indices.append((i, j))\n",
    "        edge_features.append([bond.GetBondTypeAsDouble()])  # Bond type (as double)\n",
    "    \n",
    "    edge_index = tch.tensor(edge_indices, dtype=tch.long).t().contiguous()\n",
    "    edge_attr = tch.tensor(edge_features, dtype=tch.float).view(-1, 1)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and check shape of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_smiles_data('data/solubility.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('data/X_qsar.npy')\n",
    "y = np.load('data/y_qsar.npy')\n",
    "\n",
    "\n",
    "data_pyg = df.apply(make_pyg, axis=1)\n",
    "data_pyg = data_pyg[data_pyg.apply(lambda x: len(x.edge_index.shape) != 1)]\n",
    "data_pyg.reset_index(drop=True, inplace=True)\n",
    "dbl = int(len(data_pyg) * 0.3)\n",
    "dbi = np.arange(dbl)\n",
    "data_pyg= data_pyg[dbi[:dbl]]\n",
    "data_pyg.reset_index(drop=True, inplace=True)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and test sets and standardize the data\n",
    "Here we will just have a training and test set, so our results will not be quite as rigerous as those you got with cross-validation in the supervised machine learning lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(y) * 0.7) # 70% of data for training and 30% for testing\n",
    "\n",
    "random.seed(1234)\n",
    "indices = np.arange(len(y))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# X_train0 is our training data prior to standardization\n",
    "X_train0, X_test0 = X[indices[:n_train]], X[indices[n_train:]]\n",
    "y_train, y_test = y[indices[:n_train]], y[indices[n_train:]]\n",
    "\n",
    "# standardize X_train0 and X_test0 to give X_train and X_test\n",
    "scaler = StandardScaler().fit(X_train0)\n",
    "X_train = scaler.transform(X_train0)\n",
    "X_test = scaler.transform(X_test0)\n",
    "\n",
    "\n",
    "# Here we split the molecular data into a train and a test set\n",
    "n_train = int(len(data_pyg) * 0.7) # 70% of data for training and 30% for testing\n",
    "# random.seed(1234)\n",
    "indices = np.arange(n_train)\n",
    "# print(indices[-1])\n",
    "data_train = data_pyg[indices[:n_train]]\n",
    "data_train.reset_index(drop=True, inplace=True)\n",
    "data_test = data_pyg[~data_pyg.isin(data_train)]\n",
    "data_test.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "## Random Forest Regressor & Support Vector Regressor\n",
    "For comparative purposes, with the results we will explore later with a more involved neural network architectures than the one above, we will build a Linear Regression, Random Forest and Support Vector model. For these three machine learning algorithms we will just use the default hyper parameter settings, which are often a good place to start. This means that you will just have () after the model definition, as you did for the linear regression with LinearRegression(). To change the hyper parameters from the defaults one needs to specify them within the braces.\n",
    "\n",
    "The code cells for the random forest and support vector regressors have been left blank below. You should fill in these cells. You should define the models, fit them, make predictions from them, compute their MSEs and print out the results.\n",
    "\n",
    "* hint 1: look to the cell where we 'Load packages' to get the right model definition for the two machine learning methods\n",
    "* hint 2: look at the cell with Linear Regression. It should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "LR_model = LinearRegression()\n",
    "### Add Code Here\n",
    "print('Linear Regression: MSE = ' + str(np.round(LR_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RandomForestRegressor()\n",
    "### Add Code Here\n",
    "print('Random Forest Regressor: MSE = ' + str(np.round(RF_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SV_model = SVR()\n",
    "### Add code here\n",
    "print('Support Vector Regressor: MSE = ' + str(np.round(SV_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical neural network as a linear regression\n",
    "If we define a neural network with no hidden layers and no non-linear activations we essentailly get the same results as we do with basic linear regression. The results below should help clarify that to you (there are some minor differences hovever, hence the MSE for the neural network will not be _exactly_ the same as the results above for linear regression, but they are neverthelss very close).\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/lin-reg.png\" alt=\"drawing\" style=\"width:1200px;\"/>\n",
    "    <center>Figure 1. Our neural network version of linear regression.</center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model\n",
    "You will need to convert the data into tensors and define the model to use Neural Networks. IN this particular case, you'll get some help - but keep note of how it's done, you *will* have to do it yourselves in the rest of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = tch.tensor(X_train, dtype=tch.float32).to(device)\n",
    "y_train_tensor = tch.tensor(y_train, dtype=tch.float32).to(device).unsqueeze(1)  # Add dimension for regression\n",
    "X_test_tensor = tch.tensor(X_test, dtype=tch.float32).to(device)\n",
    "y_test_tensor = tch.tensor(y_test, dtype=tch.float32).to(device).unsqueeze(1)\n",
    "# print(y_test_tensor)\n",
    "# print(y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ANN1(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ANN1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)  # Output layer with 1 nodes\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Innitialize the model and set optimizer and learning rate hyperparameters\n",
    "The learning rate and optimizer chosen below are both things that can be changed when one explores hyper parameter options, different architectures and what not. Below we use a learning rate (lr) of 0.001 (a common default learning rate) and the 'Adam' optimizer. take careful note here, you will be reimplementing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "ann1_model = ANN1(input_dim).to(device).float()\n",
    "asummary(ann1_model.to(device), input_size=(input_dim,))\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ann1_model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Here you will train the model. Take note of how the training loop looks. You will use similar loops for the rest of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "ann1_model.to(device)\n",
    "pbar = tqdm(range(num_epochs), desc=\"Epochs\")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ann1_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ann1_model(X_train_tensor.to(device))\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    ann1_model.eval()\n",
    "    with tch.no_grad():\n",
    "        val_outputs = ann1_model(X_test_tensor)\n",
    "        val_loss = criterion(val_outputs, y_test_tensor)\n",
    "        val_losses.append(val_loss.item())\n",
    "    # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix({\"Training Loss\": train_losses[-1], \"Validation Loss\": val_losses[-1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model\n",
    "Again, take careful note of this code - you will be using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "ann1_model.eval()\n",
    "with tch.no_grad():\n",
    "    y_pred = ann1_model(X_test_tensor)\n",
    "    y_pred_np = y_pred.cpu().detach().numpy()\n",
    "    y_test_np = y_test_tensor.cpu().detach().numpy()\n",
    "    ann_mse = mean_squared_error(y_test_np, y_pred_np)\n",
    "    print('ANNa Regression: MSE = {:.3f}'.format(ann_mse))\n",
    "\n",
    "# Plot training history\n",
    "plot_history(train_losses, val_losses, 'ANN1 Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going deeper with ANNs \n",
    "In the cells below we define, compile, fit and evaluate a neural network model with:\n",
    "* two hiiden layers, each with 32 neurons and non-linear activations (relu)\n",
    "* a dropout layer at the end with a dropout rate of 0.2\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/relu-activation.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
    "    <center>Figure 2. relu activation.</center>\n",
    "</p>\n",
    "\n",
    "Dropout can help to avoid overfitting, much as L1 and L2 regularizations do (as you explored in the supervise machine learning lab). In the model loss plots (below) this stops the test loss from increasing as you train for more epochs.\n",
    "\n",
    "Some quotes from a paper co-authored by members of our group called \"Deep Learning in Image Cytometry: A Review\" (https://onlinelibrary.wiley.com/doi/full/10.1002/cyto.a.23701):\n",
    "\n",
    "\"_Overfitting occurs when the parameters of a model fit too closely to the input training data, without capturing the underlying distribution, and thus reducing the model’s ability to generalize to other datasets_\".\n",
    "\n",
    "DROPOUT: \"_A regularization technique that reduces the interdependent learning among the neurons to prevent overfitting. Some neurons are randomly “dropped,” or disconnected from other neurons, at every training iteration, removing their influence on the optimization of the other neurons. Dropout creates a sparse network composed of several networks—each trained with a subset of the neurons. This transformation into an ensemble of networks hugely decreases the possibility of overfitting, and can lead to better generalization and increased accuracy_\".\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/dropout.png\" alt=\"drawing\" style=\"width:1200px;\"/>\n",
    "    <center>Figure 3. Dropout.</center>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ANN2, self).__init__()\n",
    "        ### Add Code here\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ### Add code here\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ANN2 model and summmarize it\n",
    "# input_dim = \n",
    "# asummary()\n",
    "\n",
    "# Loss and optimizer\n",
    "# criterion = \n",
    "# optimizer = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "pbar = tqdm(range(num_epochs), desc=\"Epochs\")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix({\"Training Loss\": train_losses[-1], \"Validation Loss\": val_losses[-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# ann2_model.eval()\n",
    "# with tch.no_grad():\n",
    "\n",
    "\n",
    "# Plot training history\n",
    "plot_history(train_losses, val_losses, 'ANN2 Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing GNNs\n",
    "So, now you've tested regression on molecular descriptors with ANNs. ANother option that is gaining traction in the research world is using Graph Neural Networks or, as they can also be called, Graph Convolutional Networks. You will be using an extension library called Pytorch.Geometric for this. Training and Evaluation looks the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN1(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GNN1, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, 32)\n",
    "        self.conv2 = GCNConv(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)  # Output layer with 1 node\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GNN1 model\n",
    "input_dim = data_train.iloc[5].x.size(1)\n",
    "gnn1_model = GNN1(input_dim)\n",
    "#Loss and optimizer\n",
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gnn1_model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
    "t_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "v_loader = DataLoader(data_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit GNN Model here. Hint, this is a bit harder - you need to account for batches, and averages. You have some more help-code prepared. Fix the commented sections. Take inspiration from your previous training-loop\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "pbar = tqdm(range(num_epochs), desc=\"Epochs\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase. Don't forget to save you losses! and to set your model to train!\n",
    "    # gnn1_model.train()\n",
    "\n",
    "    for batch in t_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Use Batch Data object in forward pass\n",
    "        outputs = gnn1_model(batch.x, batch.edge_index, batch.batch)\n",
    "        #Here you need some code. it should look very similar to you previous loop for ANN2\n",
    "\n",
    "\n",
    "    avg_train_loss = sum(train_loss_items) / len(train_loss_items)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    # Validation Phase (assuming you have a separate validation loader)\n",
    "    gnn1_model.eval()\n",
    "    val_loss_items = []\n",
    "    with tch.no_grad():\n",
    "        for val_batch in v_loader:\n",
    "            #here you run your validation per batch\n",
    " \n",
    "    # I suggest you deal with val_loss as you did with train_loss\n",
    "    val_losses.append(avg_val_loss)\n",
    "    # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_losses[-1]:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix({\"Training Loss\": avg_train_loss, \"Validation Loss\": avg_val_loss})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We've prepared the eval for you.\n",
    "If you did everything else correctly, this will run out-of-the-box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn1_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with tch.no_grad():\n",
    "    for v_batch in v_loader:\n",
    "        test_outputs = gnn1_model(v_batch.x, v_batch.edge_index, v_batch.batch)\n",
    "        all_preds.extend(test_outputs.tolist())\n",
    "        all_labels.extend(v_batch.y.tolist())\n",
    "all_preds_tensor = tch.tensor(all_preds)\n",
    "all_labels_tensor = tch.tensor(all_labels)\n",
    "gnn1_mse = mean_squared_error(all_labels_tensor, all_preds_tensor)\n",
    "\n",
    "print(f'GNN1 Regression: MSE = {gnn1_mse:.3f}')\n",
    "plot_history(train_losses, val_losses, 'GNN1 Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
