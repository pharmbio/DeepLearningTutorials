{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'markdown cell' below  replace the `???` with the names of those in your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: classification of cell morphology\n",
    "_by David Holmberg (2024)_\n",
    "#### Inspiration\n",
    "This step by step exercise was inspired by Andrew Ng's **Deep learning** Coursera specialization (https://www.coursera.org/specializations/deep-learning) . This assignment will take you through a machine learning workflow which includes reading datasets, creating and compiling models, training models on datasets, and predicting on the validation sets (for model comparisons). Much of the experience you accumualted during the labs from yesterday and the day before will come in useful here.\n",
    "\n",
    "#### Datasets\n",
    "The specific dataset we will use is a subset of the bbbc021v1 MoA dataset (http://mct.aacrjournals.org/content/9/6/1913) available from the Broad Bioimage Benchmark Collection (https://www.nature.com/articles/nmeth.2083). We chose to only use a subset of this data (based on only the six main MoAs) in order that the models we will fit below would not take too long to train.\n",
    "\n",
    "#### Importance\n",
    "After you've completed this assignment you will know how to develop and utilize advanced machine learning models (in this case convlutional neural networks (CNNs) applied to high content cell images). Traditional approaches for classyfing such biological cell-images involve complex workflows, with many steps requiring manual implementation. The more modern neural network approach (made possible through the better hardware available today, most notably via GPUs) can perform equally well and often better than the traditional approaches.\n",
    "\n",
    "\n",
    "#### Note 2\n",
    "For the assignment we will split the data into a training and validation set and will only optimize our performance on the validation set. Although from the work with the oral cancer data you saw that it is genrally best to keep out a test set on which to make a final evaluation of your chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "import torch as tch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "from torchsummary import summary\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "# device = tch.device(\"cuda\" if tch.cuda.is_available() else \"cpu\")\n",
    "# from tensorflow.python.util import deprecation\n",
    "# deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "# print(tf.__version__)\n",
    "# tch.backends.cudnn.enabled = False\n",
    "# tch.cuda.is_available = lambda : False\n",
    "# device = tch.device('cpu')\n",
    "!nvidia-smi | grep GeForce\n",
    "device = tch.device(\"cuda\" if tch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Don't worry too much about the code in the functions below, but you might want to go through when they are called later on so that you roughly understand what they're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset():\n",
    "    dirname = '/scratch-shared/students/DeepLearning/bbbc021v1_images'\n",
    "    x_orig = tch.zeros((660, 3, 256, 256), dtype=tch.float32)\n",
    "\n",
    "    for f in range(x_orig.shape[0]):\n",
    "        img = Image.open(dirname + '/bbbc021v1_%s.png' % str(f))\n",
    "        # img = img.convert('RGB')\n",
    "        img = np.array(img).transpose(2, 0, 1)\n",
    "        img = tch.tensor(img)\n",
    "        x_orig[f] = img\n",
    "\n",
    "    labels = pd.read_csv('bbbc021v1_labels.csv',\n",
    "                          usecols=[\"compound\", \"concentration\", \"moa\"],\n",
    "                          sep=\";\")\n",
    "    y_orig = np.array(labels['moa'])\n",
    "\n",
    "    return x_orig, y_orig\n",
    "\n",
    "\n",
    "def convert_to_one_hot(y, C):\n",
    "    moa_dict = {'Aurora kinase inhibitors': 0, 'Cholesterol-lowering': 1,\n",
    "                'Eg5 inhibitors': 2, 'Protein synthesis': 3, 'DNA replication': 4, 'DNA damage': 5}\n",
    "\n",
    "    y = np.asarray([moa_dict[item] for item in y])\n",
    "    # y = tch.eye(C)[y]\n",
    "    # y = y.type(tch.int32)\n",
    "    y = tch.tensor(y, dtype=tch.int64)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def plot_history(train_losses, val_losses, train_accuracies, val_accuracies, model_name):\n",
    "    epochs = len(train_losses)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5), facecolor='w')\n",
    "    \n",
    "    ax = fig.add_subplot(131)\n",
    "    ax.plot(range(1, epochs+1), train_losses, label='Train')\n",
    "    ax.plot(range(1, epochs+1), val_losses, label='Validation')\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    ax = fig.add_subplot(132)\n",
    "    ax.plot(range(1, epochs+1), np.log(train_losses), label='Train')\n",
    "    ax.plot(range(1, epochs+1), np.log(val_losses), label='Validation')\n",
    "    ax.set(title=model_name + ': Log model loss', ylabel='Log loss', xlabel='Epoch')\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    ax = fig.add_subplot(133)\n",
    "    ax.plot(range(1, epochs+1), train_accuracies, label='Train')\n",
    "    ax.plot(range(1, epochs+1), val_accuracies, label='Validation')\n",
    "    ax.set(title=model_name + ': Model accuracy', ylabel='Accuracy', xlabel='Epoch')\n",
    "    ax.legend(loc='lower right')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, model_name, cmap=plt.cm.Blues):\n",
    "    title = model_name + ': Confusion Matrix'\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def valid_evaluate(model, valid_loader, model_name, num_classes=6):\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with tch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = tch.max(outputs, 1)\n",
    "            # predicted = tch.nn.functional.one_hot(predicted, num_classes)\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(predicted.tolist())\n",
    "\n",
    "    class_names = ['Aur', 'Ch', 'Eg5', 'PS', 'DR', 'DS']\n",
    "    # print(f'y pred {y_pred}')\n",
    "    # print(f'y_tru {y_true}')\n",
    "\n",
    "    # y_true = [tch.tensor(label).argmax().item() for label in y_true]\n",
    "    # y_pred = [tch.tensor(pred).argmax().item() for pred in y_pred]\n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    \n",
    "    plt.figure(figsize=(15,5), facecolor='w')\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, model_name=model_name)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print('')\n",
    "    print('Classification report for validation data:')\n",
    "    print(classification_report(y_true, y_pred, digits=3, target_names=class_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset\n",
    "Read dataset, prepare with one-hot encoding, and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig, y_orig = load_dataset()\n",
    "Y = convert_to_one_hot(y_orig, 6)\n",
    "X = X_orig/255.\n",
    "n_train = 500\n",
    "random.seed(5026)\n",
    "indices = np.arange(len(Y))\n",
    "random.shuffle(indices)\n",
    "X_train, X_valid = X[indices[:n_train]], X[indices[n_train:]]\n",
    "Y_train, Y_valid = Y[indices[:n_train]], Y[indices[n_train:]]\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "valid_dataset = TensorDataset(X_valid, Y_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot samples of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell to plot few example images of the training set\n",
    "fig, ax = plt.subplots(2, 3, figsize=(14, 10))\n",
    "fig.suptitle(\"Examples of training images\", fontsize=20)\n",
    "axes = ax.ravel()\n",
    "for i in range(len(axes)):\n",
    "    # idx = np.where(np.argmax(Y_train, axis=1) == i)[0]\n",
    "    idx = np.where(Y_train == i)[0]\n",
    "    s_idx = np.random.choice(idx)\n",
    "    imgt = X_train[s_idx].numpy()\n",
    "    imgt = imgt.transpose(1, 2, 0)\n",
    "    img = (imgt*255).astype(\"uint8\")\n",
    "    axes[i].set_title(y_orig[indices][s_idx], fontsize=14)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_axis_off()\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN (LeNet inspired)\n",
    "\n",
    "You will be implementing LeNet and ResNet in this task. You will implement them using PyTorch\n",
    "\n",
    "Convolutional Neural Networks (CNNs) use convolutions instead of the normal fully connected layers, which have proven to be highly successful for image recognition tasks. By convolving filters on the input layer and outputting the results to the next layer, the CNN \"detects\" (or learns) features at different levels of abstraction throughout the network. With lower-level abstractions (like edges and blobs) in the early layers, and higher-level abstractions (like cells) in deeper layers. Figure 1 shows the LeNet inspired CNN that we will shortly turn into TensorFlow code. It has 3 convolutional layers, 3 max pooling layers, and two final fully connected layers.\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/conv.png\" alt=\"drawing\" style=\"width:1200px;\"/>\n",
    "    <center>Figure 1. LeNet inspired CNN that you will implement.</center>\n",
    "</p>\n",
    "\n",
    "\n",
    "If you're getting stuck or stumble upon problems, don't hesitate to use Google or similar to search for answers. There's plenty of information out there regarding PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Coding LeNet\n",
    "Define the parts in the cell below with your own code. You will be using pytorch like before, and they have excellent documentation. Make sure to read it if you need to. Start by defining the input shape and then try to code in the variant of LeNet shown above in Figure 1. The final line of your code should be a dense layer with a softmax activation. Once you have your code ready run the cell and see if your model summary matches ours (see below). Note, the stride of 2 for the first convolutional layer is something you should define within the braces of `nn.Conv2D(...)` along with the other arguments you are now used to. Your model needs the output final node to be `nn.Softmax(...)` Also pay attention to how the dimensions above change from one part of the model to the next - with the max pooling we don't always use the defualt pool size of `(2, 2)`. To compensate for over-fitting add a dropout layer prior to your prediction layer with a dropout rate of `0.2`. Name your model `LeNet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and sumarise the LeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=2, padding=2)\n",
    "        ### Add your innit code here\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        ### Define your forward pass here\n",
    "        \n",
    "        return x\n",
    "    \n",
    "### Fix the missing parts\n",
    "num_classes = ???\n",
    "LeNet_model = LeNet()\n",
    "input_size = ???\n",
    "LeNet_model = LeNet_model.to(device).float()\n",
    "summary(LeNet_model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did you define LeNet correctly?\n",
    "At the end of the summary you should see the following:\n",
    "\n",
    "Total params: 1,121,062\n",
    "\n",
    "Trainable params: 1,121,062\n",
    "\n",
    "Non-trainable params: 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the LeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ???\n",
    "optimizer = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and evaluate the LeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []  # List to store train accuracies\n",
    "val_accuracies = []  \n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    # Iterate through batches using DataLoader\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #optimizer, ouptubs etc. Don't forget that you criterion needs both outputs, labels to work.\n",
    "        ???\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = tch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation\n",
    "    LeNet_model.eval()  # Set model to evaluation mode\n",
    "    valid_loss = 0.0\n",
    "    correct_valid = 0\n",
    "    total_valid = 0\n",
    "    \n",
    "    with tch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            ### Add code here\n",
    "            ???\n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = tch.max(outputs.data, 1)\n",
    "            total_valid += labels.size(0)\n",
    "            correct_valid += (predicted == labels).sum().item()\n",
    "    \n",
    "    valid_loss /= len(valid_loader)\n",
    "    val_losses.append(valid_loss)\n",
    "\n",
    "    valid_accuracy = 100 * correct_valid / total_valid\n",
    "    val_accuracies.append(valid_accuracy)\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f} - Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history. Check the definition above for plot_history() to see what you need to input. \n",
    "\n",
    "\n",
    "# Evaluate on validation data and make a confusion matrix. Check the definitions above for valid_evaluate() to see what you need to input to get it to work. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple runs of the same model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models will converge at very large epoch values - however, there is a natural stocicisity to neural networks. It is therefore a good idea to run the model multiple times (preferable with new parallel copies) and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "Your probably noticed that the training was quite eratic for your ResNet model above (i.e. the training curve was quite jaggedy). We could combat this with changes to the model architecture, the batch size and learning rate. We could also perhaps improve things with data augmentation. We will not explore these modifications here though.\n",
    "\n",
    "This was a big model that we trained, with almost 5 million parameters. With only 500 images for training this was rather ambitious!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
