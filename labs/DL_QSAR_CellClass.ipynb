{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Deep Learning for QSAR and Cell Identification\n",
    "_by David Holmberg (February 2023), adapted from materials prepared by Phil Harrison (2021)_\n",
    "\n",
    "### Artificial neural networks for QSAR\n",
    "\n",
    "#### Dataset\n",
    "For this exercise we will use the same dataset of aqueous solubility of 1142 diverse chemical compounds as you previously explored during the QSAR lab last week. However, here we will only use the PhysChem descriptors.\n",
    "\n",
    "#### Modelling comparisons\n",
    "1. Compare the results of linear regression to those of a simple neural network with no hidden layers and no non-linear activation functions\n",
    "2. Compare the results of a simple neural network to those of a deeper neural network with hidden layers and non-linear .\n",
    "\n",
    "#### Aims\n",
    "* to see the link between neural networks and linear regression\n",
    "* to learn the basics of how to define, compile, fit and evaluate neural networks via TensorFlow.\n",
    "\n",
    "#### Note\n",
    "We will be using the open-source machine learning framework TensorFlow (https://www.tensorflow.org) and Keras (https://keras.io) for our neural networks. TensorFlow was developed by the Google Brain team and is a widely used machine learning frameworks in research and industry and Keras was the most popular higher-level API that runs atop TensorFlow. Today Keras has been entirely absorbed by Tensorflow, making it signidicantly easier to handle without touching the boilerplate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers, datasets\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Helper libraries\n",
    "import random\n",
    "import cv2\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "\n",
    "from tifffile import imread\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(model_history, model_name):\n",
    "    fig = plt.figure(figsize=(15,5), facecolor='w')\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(model_history.history['loss'])\n",
    "    ax.plot(model_history.history['val_loss'])\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(np.log(model_history.history['loss']))\n",
    "    ax.plot(np.log(model_history.history['val_loss']))\n",
    "    ax.set(title=model_name + ': Log model loss', ylabel='Log loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and check shape of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('data/X_qsar.npy')\n",
    "y = np.load('data/y_qsar.npy')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and test sets and standardize the data\n",
    "Here we will just have a training and test set, so our results will not be as rigerous as those you got with cross-validation in the supervised machine learning lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(y) * 0.7) # 70% of data for training and 30% for testing\n",
    "\n",
    "random.seed(1234)\n",
    "indices = np.arange(len(y))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# X_train0 is our training data prior to standardization\n",
    "X_train0, X_test0 = X[indices[:n_train]], X[indices[n_train:]]\n",
    "y_train, y_test = y[indices[:n_train]], y[indices[n_train:]]\n",
    "\n",
    "# standardize X_train0 and X_test0 to give X_train and X_test\n",
    "scaler = StandardScaler().fit(X_train0)\n",
    "X_train = scaler.transform(X_train0)\n",
    "X_test = scaler.transform(X_test0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = LinearRegression()\n",
    "LR_model.fit(X_train, y_train)\n",
    "LR_pred = LR_model.predict(X_test)\n",
    "LR_mse = mean_squared_error(y_test, LR_pred)\n",
    "print('Linear Regression: MSE = ' + str(np.round(LR_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical neural network as a linear regression\n",
    "If we define a neural network with no hidden layers and no non-linear activations we essentailly get the same results as we do with basic linear regression. The results below should help clarify that to you (there are some minor differences hovever, hence the MSE for the neural network will not be _exactly_ the same as the results above for linear regression, but they are neverthelss very close).\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/lin-reg.png\" alt=\"drawing\" style=\"width:1200px;\"/>\n",
    "    <center>Figure 1. Our neural network version of linear regression.</center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and summarise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = layers.Input(shape=X_train[0].shape)\n",
    "preds = layers.Dense(1)(inps)\n",
    "\n",
    "ANN1 = models.Model(inputs=inps, outputs=preds)\n",
    "ANN1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and fit the model\n",
    "The learning rate and optimizer chosen below are both things that can be changed when one explores hyper parameter options, different architectures and what not. Below we use a learning rate (lr) of 0.001 (a common default learning rate) and the 'RMSprop' optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "ANN1.compile(optimizer=optimizers.RMSprop(learning_rate=lr), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ANN1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, verbose=0)\n",
    "plot_history(history, 'ANN1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN1_mse = ANN1.evaluate(X_test, y_test, verbose=0)\n",
    "print('ANN1: MSE = ' + str(np.round(ANN1_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going deeper with ANNs \n",
    "In the cells below we define, compile, fit and evaluate a neural network model with:\n",
    "* two hiiden layers, each with 32 neurons and non-linear activations (relu)\n",
    "* a dropout layer at the end with a dropout rate of 0.2\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/relu-activation.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
    "    <center>Figure 2. relu activation.</center>\n",
    "</p>\n",
    "\n",
    "Dropout can help to avoid overfitting, much as L1 and L2 regularizations do (as you explored in the supervise machine learning lab). In the model loss plots (below) this stops the test loss from increasing as you train for more epochs.\n",
    "\n",
    "Some quotes from a paper I co-authored called \"Deep Learning in Image Cytometry: A Review\" (https://onlinelibrary.wiley.com/doi/full/10.1002/cyto.a.23701):\n",
    "\n",
    "\"_Overfitting occurs when the parameters of a model fit too closely to the input training data, without capturing the underlying distribution, and thus reducing the model’s ability to generalize to other datasets_\".\n",
    "\n",
    "DROPOUT: \"_A regularization technique that reduces the interdependent learning among the neurons to prevent overfitting. Some neurons are randomly “dropped,” or disconnected from other neurons, at every training iteration, removing their influence on the optimization of the other neurons. Dropout creates a sparse network composed of several networks—each trained with a subset of the neurons. This transformation into an ensemble of networks hugely decreases the possibility of overfitting, and can lead to better generalization and increased accuracy_\".\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/dropout.png\" alt=\"drawing\" style=\"width:1200px;\"/>\n",
    "    <center>Figure 3. Dropout.</center>\n",
    "</p>\n",
    "\n",
    "In what comes below there are no missing cells or code lines for you to fill in, this is simply an example. But pay attention to how the code is written below as in the jupyter notebook 'day1_part2' there will be missing parts that you will have to fill in. The code below will help you with those later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = layers.Input(shape=X_train[0].shape)\n",
    "x = layers.Dense(32, activation='relu')(inps)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "### TODO add another layer to the network here:\n",
    "\n",
    "x = layers.Dropout(0.2)(x)\n",
    "preds = layers.Dense(1)(x)\n",
    "\n",
    "ANN2 = models.Model(inputs=inps, outputs=preds)\n",
    "ANN2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "# TODO compile your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN2_mse = ANN2.evaluate(X_test, y_test, verbose=0)\n",
    "print('ANN2: MSE = ' + str(np.round(ANN2_mse, 3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above you should have gotten the lowest MSE for the neural network, followed by the random forest and then the support vector regressor. These results are however not the final say. Different hyper parameter settings for any of these machine learning algorithms could change the rankings. For neural networks there are many hyper parameters that one could explore, including the network architecture, the number of layers, the number of neurons per layer, the drop out rate(s), the learning rate and the optimizer to use. In later labs this week we will explore these, and additional, choices. A full comparison would also better be done via cross validation as our results above are also affected by the train/test splitting of the data..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN for Image-Based Cell Classification\n",
    "\n",
    "### Dataset\n",
    "We will be using a simplified subset of the data analysed in the paper \"Deep Convolutional Neural Networks For Detecting Cellular Changes Due To\n",
    "Malignancy\" by Wieslander et al. (2017) (http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w1/Wieslander_Deep_Convolutional_Neural_ICCV_2017_paper.pdf). The CNNs we'll be training in this exercise will be more basic than those used in that paper.\n",
    "\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/oral_cancer_summary.png\" alt=\"drawing\"  style=\"width:800px;\"/>\n",
    "    <center>Figure 1. Oral cancer data summary.</center>\n",
    "</p>\n",
    "\n",
    "Example cell images Weislander et al. used are shown below. These images were greyscale and 80 x 80 pixels.\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/cancer_cells.png\" alt=\"drawing\" style=\"width:800px;\"/>\n",
    "    <center>Figure 2. Example oral cancer cell images.</center>\n",
    "</p>\n",
    "\n",
    "To simplify the data for the exercise the images have been cropped into 48x48 pixels around the cell. All the images were then shuffled and allocated to the training, validation or test set. Hence, here we are solving a simpler classification task than the more robust one carried out by Weislander et al. where the data were not shuffled and cells from a given patient could only be in one of the three sets. A more robust and generalizable approach would be to do what Weislander et al. did, so keep this in mind.\n",
    "\n",
    "### Note\n",
    "Some of the code cells below are boiler plate code, for getting things in the right format, plotting various things and what not. You are encouraged to read and understand what the cells do, it is not required to complete the exercise. Other cells contain either minimal or partial code that you will need to implement. Regardless you need to run all cells below for the notebook to function.\n",
    "\n",
    "Also below we use `generators` to load the data from directories in small batches. This is useful if your dataset is bigger than fits into the GPU Memory (VRAM). GPUs are super fast for fitting neural networks, much faster than CPUs, but have limited VRAM, and limited access to system RAM. There are other ways of handling data loading, which will be useful should you decide to use deep-learning in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting and Summarizing functions\n",
    "def plot_history(model_history, model_name):\n",
    "    fig = plt.figure(figsize=(20,6), facecolor='w')\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(model_history.history['loss'])\n",
    "    ax.plot(model_history.history['val_loss'])\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['Training', 'Validation'], loc='upper left')\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(model_history.history['accuracy'])\n",
    "    ax.plot(model_history.history['val_accuracy'])\n",
    "    ax.set(title=model_name + ': Model Accuracy', \n",
    "           ylabel='Accuracy', xlabel='Epoch')\n",
    "    ax.legend(['Training', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, model_name,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    title = model_name\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_sample_images():\n",
    "    # plot 5 healthy cells form training data\n",
    "    print('random sample of healthy cells from training set')\n",
    "    print('')\n",
    "    all_cells = glob.glob(train_healthy_dir + '/' '*')\n",
    "    n_cells = len(all_cells)\n",
    "    to_plot = np.random.choice(n_cells, 5, replace=False)\n",
    "\n",
    "    plt.figure(figsize=(20, 120), facecolor='w')\n",
    "\n",
    "    for i in range(5):\n",
    "        im = imread(all_cells[to_plot[i]])\n",
    "        sub_index = 151 + i\n",
    "        plt.subplot(sub_index)\n",
    "        plt.imshow(im[:, :, 0], cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # plot 5 tumor cells form training data\n",
    "    print('')\n",
    "    print('random sample of tumor cells from training set')\n",
    "    print('')\n",
    "    all_cells = glob.glob(train_tumor_dir + '/' '*')\n",
    "    n_cells = len(all_cells)\n",
    "    to_plot = np.random.choice(n_cells, 5, replace=False)\n",
    "\n",
    "    plt.figure(figsize=(20, 120), facecolor='w')\n",
    "\n",
    "    for i in range(5):\n",
    "        im = imread(all_cells[to_plot[i]])\n",
    "        sub_index = 151 + i\n",
    "        plt.subplot(sub_index)\n",
    "        plt.imshow(im[:, :, 0], cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def data_summary():\n",
    "    print('no. training healthy images:', len(os.listdir(train_healthy_dir)))\n",
    "    print('no. training tumor images:', len(os.listdir(train_tumor_dir)))\n",
    "    print('')\n",
    "    print('no. validation healthy images:', len(os.listdir(validation_healthy_dir)))\n",
    "    print('no. validation tumor images:', len(os.listdir(validation_tumor_dir)))\n",
    "    print('')\n",
    "    print('no. test healthy images:', len(os.listdir(test_healthy_dir)))\n",
    "    print('no. test tumor images:', len(os.listdir(test_tumor_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory we will store our dataset\n",
    "base_dir = 'HPV/data_v3'\n",
    "\n",
    "# directories for our training, validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# directory with our training healthy cell images\n",
    "train_healthy_dir = os.path.join(train_dir, 'healthy')\n",
    "\n",
    "# directory with our training tumor cell images\n",
    "train_tumor_dir = os.path.join(train_dir, 'tumor')\n",
    "\n",
    "# directory with our validation healthy cell images\n",
    "validation_healthy_dir = os.path.join(validation_dir, 'healthy')\n",
    "\n",
    "# directory with our validation tumor cell images\n",
    "validation_tumor_dir = os.path.join(validation_dir, 'tumor')\n",
    "\n",
    "# directory with our test healthy cell images\n",
    "test_healthy_dir = os.path.join(test_dir, 'healthy')\n",
    "\n",
    "# directory with our test tumor cell images\n",
    "test_tumor_dir = os.path.join(test_dir, 'tumor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = 48\n",
    "y_len = 48\n",
    "batch_size = 32\n",
    "n_epochs = 50\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print('TRAINING DATA:')\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(x_len, y_len),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary')\n",
    "\n",
    "print('')\n",
    "print('VALIDATION DATA:')\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(x_len, y_len),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary',\n",
    "    shuffle=False)\n",
    "\n",
    "print('')\n",
    "print('TEST DATA:')\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(x_len, y_len),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary',\n",
    "    shuffle=False)\n",
    "\n",
    "train_steps = (len(os.listdir(train_healthy_dir)) + len(os.listdir(train_tumor_dir))) // batch_size\n",
    "validation_steps = (len(os.listdir(validation_healthy_dir)) + len(os.listdir(validation_tumor_dir))) // batch_size\n",
    "test_steps = (len(os.listdir(test_healthy_dir)) + len(os.listdir(test_tumor_dir))) // batch_size\n",
    "    \n",
    "def valid_evaluate(model):\n",
    "    y_pred = model.predict_generator(validation_generator, validation_steps+1)\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_true = validation_generator.classes\n",
    "    class_names = ['healthy', 'tumor']\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.figure(figsize=(15,5), facecolor='w')\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, model_name='confusion matrix for validation data')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print('')\n",
    "    print('classification report for validation data:')\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "def test_evaluate(model):\n",
    "    y_pred = model.predict_generator(test_generator, test_steps+1)\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_true = test_generator.classes\n",
    "    class_names = ['healthy', 'tumor']\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.figure(figsize=(15,5), facecolor='w')\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, model_name='confusion matrix for test data')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print('')\n",
    "    print('classification report for test data:')\n",
    "    print(classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic convolutional neural network (CNN)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) use convolutions instead of the normal fully connected layers, which have proven to be highly successful for 2D-data tasks. By convolving filters on the input layer and outputting the results to the next layer, the CNN \"detects\" (or learns) features at different levels of abstraction throughout the network. With lower-level abstractions (like edges and blobs) in the early layers, and higher-level abstractions (like cells) in deeper layers. The figure below shows the LeNet inspired CNN that we will shortly be implementing.\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/CNN.png\" alt=\"drawing\" style=\"width:1200px;\"/>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example CNN\n",
    "The following cells briefly show how to define, compile, and train a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = layers.Input((x_len, y_len, 1)) # input image dimensions\n",
    "\n",
    "# convolution and pooling layers\n",
    "x = layers.Conv2D(16, kernel_size=(3, 3), padding='same', activation='relu')(in_data)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# fully connected layers\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "out = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "CNN1 = models.Model(inputs=in_data, outputs=out)\n",
    "CNN1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN1.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = CNN1.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=n_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    verbose=2)\n",
    "\n",
    "plot_history(history, 'CNN1')\n",
    "\n",
    "valid_evaluate(CNN1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## your own architectures\n",
    "In the code cells below try some alternative model architectures. Use the 'weighted avg f1-score' on the validation data to compare different model architectures and to decide which one was best.\n",
    "\n",
    "* try fitting two different models with different numbers of convolutional layers (you can have between two and five for this data, more than five convolutional layers does not work for this data due to the shrinking of the spatial dimensions of the filter maps as they go through max pooling)\n",
    "* you can also play around with the number of filters for each convolutional layer\n",
    "* and you can experiment with different values for the dropout rate, or with no dropout at all (by either setting the dropout rate to 0.0 or removing the dropout code line all together)\n",
    "* finally, you are free to explore different numbers of neurons for the final dense layer (just before our prediction (preds) layer).\n",
    "\n",
    "#### Notes: \n",
    "* if your model does not seem to have converged after 25 epochs (the default value) you can also raise the number of epochs to train for.\n",
    "* name your model CNN2, CNN3 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Define you own CNN here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Compile your CNN here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Train and Evaluate your CNN here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding data augmentation\n",
    "\n",
    "Data augmentation is a way to try improving the performance of a model by artificially inflating the amount of annotated training data. A common approach of doing this is by either mirroring or rotating the data. \n",
    "\n",
    "We will use the same 8x data augmentation as used by Wieslander et al.:\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/augmentation.png\" alt=\"drawing\" style=\"width:1000px;\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add the data augmentations above via training data generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=90,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(x_len, y_len),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Copy your CNN here and rename it to something else. for example, CNN_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Compile your new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Train you new model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "Besides using dropout we can also explore L1 and/or L2 regularization. To add regularization to you model you add ```kernel_regularizer=l2(0.0001)``` to your layer definitions.\n",
    "i.e you turn\n",
    "\n",
    "```x = layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu')(x)```\n",
    "\n",
    "into \n",
    "\n",
    "```x = layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.0001))(x)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO define your new model, name it something new. I.e CNN_L2. You are free to name it something else, including CNN_L1 if you feel like using l1 regularisation instead:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Compile your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Train your model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Evaluation of the models\n",
    "\n",
    "You should now evaluate the innitial example model and the three models you've defined. The code for evaluating has been provided, and you only need to call on it using the correct funtion name and function inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here you will use the provided code to evaluate the innitial model>\n",
    "test_evaluate(CNN1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate your own model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate your own model with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate your own model with regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Notes\n",
    "\n",
    "You've now finished the lab. Due to the stochastic nature of how Deep Learning works, you have likely all gotten different results. This is expected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
