{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Part 3: Segmentation of Human Blood Cells using Convolutional Neural Networks\n",
    "*By David Holmberg*\n",
    "\n",
    "Welcome to Assignment part 3! In this assignment you will segment blood cells using some both scratch-built models and compare to pre-defined downloaded models.\n",
    "\n",
    "Hand-in: this notebook or a pdf of it.\n",
    "\n",
    "For this lab, we use the [Human White Blood Cell images](https://github.com/zxaoyou/segmentation_WBC) from [Jiangxi Tecom Science Corporation, China](http://en.tecom-cn.com/).\n",
    "\n",
    "The dataset contains three hundred 120x120 RGB images with one blood cell per image, and corresponding segmentation masks. The segmentation mask was manually sketched by domain experts, with the background, cytoplasms and nuclei pixels labelled as 0, 1 and 2 respectively.\n",
    "\n",
    "<img src=\"Illustrations/Dataset_1.png\" title=\"Blood cells dataset\" align=\"left\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images and masks are in the **LabData/bloodcells_seg/** folder:\n",
    "```\n",
    "└── LabData\n",
    "    └── bloodcells_seg\n",
    "        ├── masks\n",
    "        │   ├── all\n",
    "        └── images\n",
    "            ├── all\n",
    "```\n",
    "\n",
    "We want to use convolutional neural networks to do pixel-wise classification of these blood cells images into background / cytoplasm / nuclei.\n",
    "\n",
    "This assignment is based on and borrows *heavily* from a lab written by Christophe Avenel at NBIS, and his code is available here: https://github.com/NBISweden/workshop-neural-nets-and-deep-learning/tree/master/session_convolutionalNeuralNetworks/Labs\n",
    "\n",
    "So please remember that he is the original author of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import useful modules like numpy and Keras layers\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# cnn_helper contains some useful functions for this lab\n",
    "import cnn_helper\n",
    "from IPython.display import IFrame\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"]=\"tf.keras\"\n",
    "import segmentation_models as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load both the images and the masks as generators, and combine both generators into a zip of generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "NUM_CLASSES = 3 # Background, cytoplasm, nuclei\n",
    "N_CHANNELS = 3  # R,G,B\n",
    "BATCH_SIZE = 8  # You can tune this and see which batch size runs faster / gives the best accuracy.\n",
    "\n",
    "\n",
    "train_steps = 240/BATCH_SIZE\n",
    "val_steps = 64/BATCH_SIZE\n",
    "\n",
    "\n",
    "seed = 909 # (IMPORTANT) to transform image and corresponding mask with same augmentation parameter.\n",
    "image_datagen = ImageDataGenerator(\n",
    "    validation_split=0.2,\n",
    "    rescale=1./255 #We need to rescale png images from 0-255 integers to 0.0-1.0 floats.\n",
    ")\n",
    "mask_datagen = ImageDataGenerator(\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Load image and mask generators for training:\n",
    "train_image_generator = image_datagen.flow_from_directory(\n",
    "    directory='LabData/bloodcells_seg/images/',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    class_mode=None, \n",
    "    seed=seed,\n",
    "    subset='training',\n",
    "    color_mode='rgb',\n",
    "    shuffle=False)\n",
    "# Note: the class_mode is used when the class is extracted from the directory\n",
    "# structure. Here the class will be defined by the mask from the generator below, \n",
    "# so we add class_mode: None\n",
    "\n",
    "train_mask_generator = mask_datagen.flow_from_directory(\n",
    "    directory='LabData/bloodcells_seg/masks/',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    subset='training',\n",
    "    color_mode='grayscale',\n",
    "    shuffle=False)\n",
    "\n",
    "train_generator = zip(train_image_generator, train_mask_generator)\n",
    "\n",
    "# Load image and mask  generators for validation:\n",
    "val_image_generator = image_datagen.flow_from_directory(\n",
    "    directory='LabData/bloodcells_seg/images/',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    subset='validation',\n",
    "    color_mode='rgb',\n",
    "    shuffle=False)\n",
    "\n",
    "val_mask_generator = mask_datagen.flow_from_directory(\n",
    "    directory='LabData/bloodcells_seg/masks/',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    subset='validation',\n",
    "    color_mode='grayscale',\n",
    "    shuffle=False)\n",
    "\n",
    "val_generator = zip(val_image_generator, val_mask_generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we're defining the U-NET model. This model will break down the image into it's base components and reconstruct it into masks. Then we can use it to create binary masks for all cells for future segmentation.\n",
    "\n",
    "\n",
    "<img src=\"Illustrations/unet.png\" title=\"Unet model\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input((IMG_SIZE, IMG_SIZE, N_CHANNELS))\n",
    "\n",
    "c1 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (inputs)\n",
    "c1 = layers.Dropout(0.1) (c1)\n",
    "c1 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c1)\n",
    "p1 = layers.MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)\n",
    "c2 = layers.Dropout(0.1) (c2)\n",
    "c2 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "p2 = layers.MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "c3 = layers.Dropout(0.2) (c3)\n",
    "c3 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c3)\n",
    "p3 = layers.MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "c4 = layers.Dropout(0.2) (c4)\n",
    "c4 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4)\n",
    "p4 = layers.MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)\n",
    "c5 = layers.Dropout(0.3) (c5)\n",
    "c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c5)\n",
    "\n",
    "u6 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "u6 = layers.concatenate([u6, c4])\n",
    "c6 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6)\n",
    "c6 = layers.Dropout(0.2) (c6)\n",
    "c6 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6)\n",
    "\n",
    "u7 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = layers.concatenate([u7, c3])\n",
    "c7 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u7)\n",
    "c7 = layers.Dropout(0.2) (c7)\n",
    "c7 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "\n",
    "u8 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = layers.concatenate([u8, c2])\n",
    "c8 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u8)\n",
    "c8 = layers.Dropout(0.1) (c8)\n",
    "c8 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c8)\n",
    "\n",
    "u9 = layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = layers.concatenate([u9, c1], axis=3)\n",
    "c9 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u9)\n",
    "c9 = layers.Dropout(0.1) (c9)\n",
    "c9 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c9)\n",
    "\n",
    "outputs = layers.Conv2D(NUM_CLASSES, (1, 1), activation='softmax') (c9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While **RELU** is the goto-function, there are other options. Typically you'd want to chose anything from the **RELU** family of activations functions (relu, selu, relu6, elu).\n",
    "\n",
    "You can visualize and compare activation functions with this online tool from [Justin Emery](https://tech.courses/author/justinjustinemery-co-uk/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame('https://polarisation.github.io/tfjs-activation-functions/', width=860, height=470)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Keras SparseCategoricalCrossentropy loss function (see https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy).\n",
    "\n",
    "because our ground truth is represented by integers on a mask (0 for background, 1 for cytoplasm, 2 for nuclei pixels) this becomes a good choice.\n",
    "\n",
    "Please define, summarize, and compile it in the following three cells:\n",
    "\n",
    "**tip:** you'll want to define the loss specifically, i.e use tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False) and not 'SparseCategoricalCrossentropy' as before.\n",
    "\n",
    "**tip:** you can define the learning rate specifically in Adam() inside the parenthesis. I suggest you start with 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define model here:\n",
    "model = keras.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize model here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model here using optimizer=Adam and loss=tf.keras.losses.SparseCategoricalCrossentropy (Hint; Google it!). You use the compilation the same way as in previous assignments:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will train the network.\n",
    "\n",
    "Look back at how you trained your model in the previous Assignment.\n",
    "\n",
    "You can track the progress of your training using the callback function we've defined for you in the cell below. you can do this by adding ```callbacks=[plot_callback]``` at the end of your model.fit_generator function. Since this is the first time you're using unsupervised, I've provided the fit_generator function for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot callback:\n",
    "epochs = ???\n",
    "\n",
    "plot_callback = cnn_helper.PlottingKerasCallback(\n",
    "    test_batch=next(val_generator),\n",
    "    num_plot=3 # how many validation examples to plot at each epoch (maximum = BATCH_SIZE)\n",
    ")\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=train_generator,\n",
    "    steps_per_epoch= train_steps,\n",
    "    validation_data= val_generator,\n",
    "    validation_steps= val_steps,\n",
    "    epochs= epochs,\n",
    "    callbacks=[plot_callback]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following cell to batch plot som validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_helper.plot_prediction(model, next(val_generator), BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Try improving on your model by changing the learning rate, epochs, activation function or any other parameters you want. I suggest you stick to the training parameters to begin with, as they are easier to tune.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models\n",
    "\n",
    "To simplify the construction of your networks, you will now use the `segmentation_model` python library ( it should be installed already, but if you get an error, install using `pip install segmentation_model`).\n",
    " \n",
    "You can read more about the `segmentation_model` library at https://github.com/qubvel/segmentation_models.\n",
    "\n",
    "## List of models:\n",
    "\n",
    "| Unet | Linknet |\n",
    "| --- | --- |\n",
    "| <img width=\"500px\" src=\"https://raw.githubusercontent.com/qubvel/segmentation_models/master/images/unet.png\" title=\"UNet\"/> | <img width=\"500px\" src=\"https://github.com/qubvel/segmentation_models/raw/master/images/linknet.png\" title=\"Linknet\"/> |\n",
    "\n",
    "| PSPNet | FPN |\n",
    "| --- | --- |\n",
    "| <img width=\"500px\" src=\"https://github.com/qubvel/segmentation_models/raw/master/images/pspnet.png\" title=\"UNet\"/> | <img width=\"500px\" src=\"https://github.com/qubvel/segmentation_models/raw/master/images/fpn.png\" title=\"Linknet\"/> |\n",
    "\n",
    "## List of backbones:\n",
    "\n",
    "| Type | Names |\n",
    "| --- | --- |\n",
    "| VGG | `vgg16` `vgg19` |\n",
    "| ResNet | `resnet18` `resnet34` `resnet50` `resnet101` `resnet152` |\n",
    "| SE-ResNet | `seresnet18` `seresnet34` `seresnet50` `seresnet101` `seresnet152` |\n",
    "| ResNeXt | `resnext50` `resnext101` |\n",
    "| SE-ResNeXt | `seresnext50` `seresnext101` |\n",
    "| SENet154 | `senet154` |\n",
    "| DenseNet | `densenet121` `densenet169` `densenet201` |\n",
    "| Inception | `inceptionv3` `inceptionresnetv2` |\n",
    "| MobileNet | `mobilenet` `mobilenetv2` |\n",
    "| EfficientNet | `efficientnetb0` `efficientnetb1` `efficientnetb2` `efficientnetb3` `efficientnetb4` `efficientnetb5` `efficientnetb6` `efficientnetb7` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try repeating the segmentation using a combination of either UNET or LinkNet with one of the backbones. To save your from the headache of trying to use an unfamiliar python package we've provided an example of using UNET with the VGG16 back-end you used in the previous assignment.\n",
    "\n",
    "Try playing around a bit. Unet and Linknet are used for the type of tasks we are performing here, and as such I recommend you to use them here as well.\n",
    "\n",
    "PN (Feature Pyramid Networks) are mainly used for object detection, and PSPNet (Pyramid Scene Parsing Networks) are not covered here, but you can read about them on https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the Unet model, using the VGG16 backbone\n",
    "model = sm.Unet(\n",
    "    'vgg16',\n",
    "    encoder_weights=None,\n",
    "    classes=NUM_CLASSES,\n",
    "    activation='softmax',\n",
    "    input_shape=(IMG_SIZE,IMG_SIZE,3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the model summary:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You will now define, compile, and train the new model you selected from the module with the same parameters as your best run with your self-built model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile here, same way as before. Same optimizer, same loss function:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model here. Hint; you can use the same fit_generator function as before. Just make sure to rename the variables correctly!:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print prediction the same way as before:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Assignment part 3:\n",
    "\n",
    "And this is how image segmentation is done. I hope you enjoyed this brief introduction, and encourage you to play around and test multiple models, both predefined and scratch-built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
