{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neural networks for QSAR\n",
    "_by David Holmberg (February 2023)_\n",
    "#### Dataset\n",
    "For this exercise we will use the same dataset of aqueous solubility of 1142 diverse chemical compounds as you previously explored during the QSAR lab last week. However, here we will only use the PhysChem descriptors.\n",
    "\n",
    "#### Modelling comparisons\n",
    "1. Compare the results of linear regression to those of a simple neural network with no hidden layers and no non-linear activation functions\n",
    "2. Compare the results of a a random forest regressor, a support vector regressor, and a neural network with two hidden layers (with non-linear activations) and dropout.\n",
    "\n",
    "#### Aims\n",
    "* to see the link between neural networks and linear regression\n",
    "* to learn the basics of how to define, compile, fit and evaluate neural networks via TensorFlow.\n",
    "\n",
    "#### Note\n",
    "We will be using the open-source machine learning framework TensorFlow (https://www.tensorflow.org) and Keras (https://keras.io) for our neural networks. TensorFlow was developed by the Google Brain team and is today one of the most widely used machine learning frameworks in research and industry and Keras was/is the most popular higher-level API that runs atop TensorFlow. However, last year TensorFlow 2 was released. In TensorFow 2 (which we will use for all our neural network work) Keras is now fully integrated. This means that we get all the benefits of TensorFlow with a much easier (Keras-type) way to define and train models than was previously possible with TensorFlow 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Pytorch and Pytorch Geometric\n",
    "import torch as tch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "from torchsummary import summary\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions\n",
    "Run these cells to have access to the necessary functions for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_history(train_losses, val_losses, model_name):\n",
    "    fig = plt.figure(figsize=(15, 5), facecolor='w')\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(train_losses)\n",
    "    ax.plot(val_losses)\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(np.log(train_losses))\n",
    "    ax.plot(np.log(val_losses))\n",
    "    ax.set(title=model_name + ': Log model loss', ylabel='Log loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and check shape of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('data/X_qsar.npy')\n",
    "y = np.load('data/y_qsar.npy')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and test sets and standardize the data\n",
    "Here we will just have a training and test set, so our results will not be quite as rigerous as those you got with cross-validation in the supervised machine learning lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(y) * 0.7) # 70% of data for training and 30% for testing\n",
    "\n",
    "random.seed(1234)\n",
    "indices = np.arange(len(y))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# X_train0 is our training data prior to standardization\n",
    "X_train0, X_test0 = X[indices[:n_train]], X[indices[n_train:]]\n",
    "y_train, y_test = y[indices[:n_train]], y[indices[n_train:]]\n",
    "\n",
    "# standardize X_train0 and X_test0 to give X_train and X_test\n",
    "scaler = StandardScaler().fit(X_train0)\n",
    "X_train = scaler.transform(X_train0)\n",
    "X_test = scaler.transform(X_test0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "## Random Forest Regressor & Support Vector Regressor\n",
    "For comparative purposes, with the results we will explore later with a more involved neural network architectures than the one above, we will build a Linear Regression, Random Forest and Support Vector model. For these three machine learning algorithms we will just use the default hyper parameter settings, which are often a good place to start. This means that you will just have () after the model definition, as you did for the linear regression with LinearRegression(). To change the hyper parameters from the defaults one needs to specify them within the braces.\n",
    "\n",
    "The code cells for the random forest and support vector regressors have been left blank below. You should fill in these cells. You should define the models, fit them, make predictions from them, compute their MSEs and print out the results.\n",
    "\n",
    "* hint 1: look to the cell where we 'Load packages' to get the right model definition for the two machine learning methods\n",
    "* hint 2: look at the cell with Linear Regression. It should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "LR_model = LinearRegression()\n",
    "LR_model.fit(X_train, y_train)\n",
    "LR_pred = LR_model.predict(X_test)\n",
    "LR_mse = mean_squared_error(y_test, LR_pred)\n",
    "print('Linear Regression: MSE = ' + str(np.round(LR_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RandomForestRegressor()\n",
    "RF_model.fit(X_train, y_train)\n",
    "RF_pred = RF_model.predict(X_test)\n",
    "RF_mse = mean_squared_error(y_test, RF_pred)\n",
    "print('Random Forest Regressor: MSE = ' + str(np.round(RF_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SV_model = SVR()\n",
    "SV_model.fit(X_train, y_train)\n",
    "SV_pred = SV_model.predict(X_test)\n",
    "SV_mse = mean_squared_error(y_test, SV_pred)\n",
    "print('Support Vector Regressor: MSE = ' + str(np.round(SV_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical neural network as a linear regression\n",
    "If we define a neural network with no hidden layers and no non-linear activations we essentailly get the same results as we do with basic linear regression. The results below should help clarify that to you (there are some minor differences hovever, hence the MSE for the neural network will not be _exactly_ the same as the results above for linear regression, but they are neverthelss very close).\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/lin-reg.png\" alt=\"drawing\" style=\"width:1200px;\"/>\n",
    "    <center>Figure 1. Our neural network version of linear regression.</center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model\n",
    "You will need to convert the data into tensors and define the model to use Neural Networks. IN this particular case, you'll get some help - but keep note of how it's done, you *will* have to do it yourselves in the rest of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = tch.tensor(X_train, dtype=tch.float32)\n",
    "y_train_tensor = tch.tensor(y_train, dtype=tch.float32).unsqueeze(1)  # Add dimension for regression\n",
    "X_test_tensor = tch.tensor(X_test, dtype=tch.float32)\n",
    "y_test_tensor = tch.tensor(y_test, dtype=tch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ANN1(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ANN1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)  # Output layer with 8 nodes\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Innitialize the model and set optimizer and learning rate hyperparameters\n",
    "The learning rate and optimizer chosen below are both things that can be changed when one explores hyper parameter options, different architectures and what not. Below we use a learning rate (lr) of 0.001 (a common default learning rate) and the 'Adam' optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "ann_model = ANN1(input_dim)\n",
    "summary(ann1_model, input_size=(input_dim,))\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ann_model.parameters(), lr=0.001)  # Adjust learning rate as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Here you will train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000  # Adjust as needed\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    ann_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ann_model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    ann_model.eval()\n",
    "    with tch.no_grad():\n",
    "        val_outputs = ann_model(X_test_tensor)\n",
    "        val_loss = criterion(val_outputs, y_test_tensor)\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "ann_model.eval()\n",
    "with tch.no_grad():\n",
    "    y_pred = ann_model(X_test_tensor)\n",
    "    ann_mse = mean_squared_error(y_test_tensor, y_pred)\n",
    "    print('ANN Regression: MSE = {:.3f}'.format(ann_mse))\n",
    "\n",
    "# Plot training history\n",
    "plot_history(train_losses, val_losses, 'ANN Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going deeper with ANNs \n",
    "In the cells below we define, compile, fit and evaluate a neural network model with:\n",
    "* two hiiden layers, each with 32 neurons and non-linear activations (relu)\n",
    "* a dropout layer at the end with a dropout rate of 0.2\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/relu-activation.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
    "    <center>Figure 2. relu activation.</center>\n",
    "</p>\n",
    "\n",
    "Dropout can help to avoid overfitting, much as L1 and L2 regularizations do (as you explored in the supervise machine learning lab). In the model loss plots (below) this stops the test loss from increasing as you train for more epochs.\n",
    "\n",
    "Some quotes from a paper co-authored by members of our group called \"Deep Learning in Image Cytometry: A Review\" (https://onlinelibrary.wiley.com/doi/full/10.1002/cyto.a.23701):\n",
    "\n",
    "\"_Overfitting occurs when the parameters of a model fit too closely to the input training data, without capturing the underlying distribution, and thus reducing the model’s ability to generalize to other datasets_\".\n",
    "\n",
    "DROPOUT: \"_A regularization technique that reduces the interdependent learning among the neurons to prevent overfitting. Some neurons are randomly “dropped,” or disconnected from other neurons, at every training iteration, removing their influence on the optimization of the other neurons. Dropout creates a sparse network composed of several networks—each trained with a subset of the neurons. This transformation into an ensemble of networks hugely decreases the possibility of overfitting, and can lead to better generalization and increased accuracy_\".\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/dropout.png\" alt=\"drawing\" style=\"width:1200px;\"/>\n",
    "    <center>Figure 3. Dropout.</center>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ANN2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 8)  # Output layer with 8 nodes\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ANN2 model\n",
    "input_dim = X_train.shape[1]\n",
    "ann2_model = ANN2(input_dim)\n",
    "summary(ann2_model, input_size=(input_dim,))\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ann2_model.parameters(), lr=0.001)  # Adjust learning rate as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000  # Adjust as needed\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    ann2_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ann2_model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    ann2_model.eval()\n",
    "    with tch.no_grad():\n",
    "        val_outputs = ann2_model(X_test_tensor)\n",
    "        val_loss = criterion(val_outputs, y_test_tensor)\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann2_model.eval()\n",
    "with tch.no_grad():\n",
    "    y_pred = ann2_model(X_test_tensor)\n",
    "    ann2_mse = mean_squared_error(y_test_tensor, y_pred)\n",
    "    print('ANN2 Regression: MSE = {:.3f}'.format(ann2_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing GNNs\n",
    "So, now you've tested regression on molecular descriptors with ANNs. ANother option that is gaining traction in the research world is using Graph Neural Networks or, as they can also be called, Graph Convolutional Networks. You will be using an extension library called Pytorch.Geometric for this. Training and Evaluation looks the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN1(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GNN1, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, 32)\n",
    "        self.conv2 = GCNConv(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 8)  # Output layer with 8 nodes\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GNN1 model\n",
    "input_dim = X_train.shape[1]\n",
    "gnn1_model = GNN1(input_dim)\n",
    "summary(gnn1_model, input_size=(input_dim,))\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gnn1_model.parameters(), lr=0.001)  # Adjust learning rate as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000  # Adjust as needed\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    gnn1_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = gnn1_model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    ann2_model.eval()\n",
    "    with tch.no_grad():\n",
    "        val_outputs = gnn1_model(X_test_tensor)\n",
    "        val_loss = criterion(val_outputs, y_test_tensor)\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn1_model.eval()\n",
    "with tch.no_grad():\n",
    "    y_pred = ann2_model(X_test_tensor)\n",
    "    ann2_mse = mean_squared_error(y_test_tensor, y_pred)\n",
    "    print('GNN1 Regression: MSE = {:.3f}'.format(ann2_mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
